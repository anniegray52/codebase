{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.sparse.linalg import svds\n",
    "import networkx as nx\n",
    "from copy import deepcopy\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finished functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_from_dataframe(data, partition_pairs, join_token='::'):\n",
    "    \"\"\"\n",
    "    Create a DMP graph from a dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame or list of pandas.DataFrame\n",
    "        The data to be used to create the graph. If a list of dataframes is\n",
    "        passed, each dataframe is used to create a separate graph.\n",
    "    partition_pairs : list of lists\n",
    "        The partition pairs to be used to create the graph. Each element of\n",
    "        the list is a list of two elements, which are the names of the\n",
    "        partitions to be joined.\n",
    "    join_token : str\n",
    "        The token used to join the partition name and the node name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : scipy.sparse.csr_matrix\n",
    "        The adjacency matrix of the graph.\n",
    "    attributes : list of lists\n",
    "        The attributes of the nodes. The first list contains the attributes\n",
    "        of the nodes that do not change over time. The second list contains\n",
    "        the attributes of the nodes that change over time.\n",
    "    \"\"\"\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    if not isinstance(partition_pairs[0][0], list):\n",
    "        partition_pairs = [partition_pairs]\n",
    "        \n",
    "    edge_list = []\n",
    "    for data0, partition_pairs0 in zip(data, partition_pairs):\n",
    "        for partition_pair in partition_pairs0:\n",
    "            pair_data = data0[partition_pair].drop_duplicates()\n",
    "            pair_data.columns = ['V1', 'V2']\n",
    "            pair_data['V1'] = [partition_pair[0] + join_token + str(x) for x in pair_data['V1']]\n",
    "            pair_data['V2'] = [partition_pair[1] + join_token + str(x) for x in pair_data['V2']]\n",
    "            pair_data['P1'] = partition_pair[0]\n",
    "            pair_data['P2'] = partition_pair[1]\n",
    "            edge_list.append(pair_data)\n",
    "            \n",
    "    edge_list = pd.concat(edge_list)\n",
    "    nodes = sorted(list(set(list(edge_list['V1']) + list(edge_list['V2']))))\n",
    "    node_ids = {x: i for i, x in enumerate(nodes)}\n",
    "    partitions = [str.split(x, join_token)[0] for x in nodes]\n",
    "    n_nodes = len(nodes)\n",
    "    \n",
    "    edge_list['V_ID1'] = [node_ids[x] for x in edge_list['V1']]\n",
    "    edge_list['V_ID2'] = [node_ids[x] for x in edge_list['V2']]\n",
    "    \n",
    "    A = sparse.coo_matrix((np.ones(2*len(edge_list)),\n",
    "                    (pd.concat([edge_list['V_ID1'], edge_list['V_ID2']]),\n",
    "                     pd.concat([edge_list['V_ID2'], edge_list['V_ID1']]))),\n",
    "                    shape=[n_nodes, n_nodes])\n",
    "    \n",
    "    attributes = [[{'name': name, 'partition': partition} for name, partition in zip(nodes, partitions)]] * 2\n",
    "\n",
    "    A = A.tocsr()\n",
    "    return A, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realise this may seem like a convoluted way to do this but it'll make it easier to add time element later\n",
    "def find_subgraph(A, attributes, subgraph_attributes):\n",
    "    \"\"\"\n",
    "    Find a subgraph of a multipartite graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix\n",
    "        The adjacency matrix of the multipartite graph.\n",
    "    attributes : list of lists\n",
    "        The attributes of the nodes. The first list contains the attributes\n",
    "        of the nodes in rows. The second list contains\n",
    "        the attributes of the nodes in the columns.\n",
    "    subgraph_attributes : list of lists\n",
    "        The attributes of the nodes of the wanted in the subgraph. The first list contains\n",
    "        the attributes of the nodes wanted in the rows. The second\n",
    "        list contains the attributes of the nodes wanted in the column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    subgraph_A : scipy.sparse.csr_matrix\n",
    "        The adjacency matrix of the subgraph.\n",
    "    subgraph_attributes : list of lists\n",
    "        The attributes of the nodes of the subgraph. The first list contains\n",
    "        the attributes of the nodes in the rows. The second\n",
    "        list contains the attributes of the nodes in the columns.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(subgraph_attributes[0], list):\n",
    "        subgraph_attributes[0] = [subgraph_attributes[0]]\n",
    "\n",
    "    if not isinstance(subgraph_attributes[1], list):\n",
    "        subgraph_attributes[1] = [subgraph_attributes[1]]\n",
    "\n",
    "    # find the indices of the rows with required attributes\n",
    "    subgraph_node_indices_row = []\n",
    "    for node_idx, node_attributes in enumerate(attributes[0]):\n",
    "        for each_subgraph_attributes in subgraph_attributes[0]:\n",
    "            matched = True\n",
    "            for key, value in each_subgraph_attributes.items():\n",
    "                if key not in node_attributes or node_attributes[key] != value:\n",
    "                    matched = False\n",
    "                    break\n",
    "            if matched:\n",
    "                subgraph_node_indices_row.append(node_idx)\n",
    "\n",
    "    # find the indices of the columns with required attributes\n",
    "    subgraph_node_indices_col = []\n",
    "    for node_idx, node_attributes in enumerate(attributes[1]):\n",
    "        for each_subgraph_attributes in subgraph_attributes[1]:\n",
    "            matched = True\n",
    "            for key, value in each_subgraph_attributes.items():\n",
    "                if key not in node_attributes or node_attributes[key] != value:\n",
    "                    matched = False\n",
    "                    break\n",
    "            if matched:\n",
    "                subgraph_node_indices_col.append(node_idx)\n",
    "\n",
    "    # create subgraph and subgraph attributes\n",
    "    subgraph_A = A[np.ix_(subgraph_node_indices_row,\n",
    "                          subgraph_node_indices_col)]\n",
    "    subgraph_attributes = [[attributes[0][i] for i in subgraph_node_indices_row], [\n",
    "        attributes[1][i] for i in subgraph_node_indices_col]]\n",
    "\n",
    "    return subgraph_A, subgraph_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_matrix(m, n = None):\n",
    "    \"\"\"\n",
    "    Create a zero matrix.\n",
    "    \"\"\"\n",
    "    if n == None:\n",
    "        n = m\n",
    "    M = sparse.coo_matrix(([],([],[])),shape = (m,n))\n",
    "    return M\n",
    "\n",
    "def symmetric_dilation(M):\n",
    "    \"\"\"\n",
    "    Dilate a matrix to a symmetric matrix.\n",
    "    \"\"\"\n",
    "    m, n = M.shape\n",
    "    D = sparse.vstack([sparse.hstack([zero_matrix(m), M]), sparse.hstack([M.T, zero_matrix(n)])])\n",
    "    return D\n",
    "\n",
    "def subgraph_idx(A, attributes, idx0, idx1):\n",
    "    \"\"\"\n",
    "    Find a subgraph of a multipartite graph by indices.\n",
    "    \"\"\"  \n",
    "    subgraph_A = A[np.ix_(idx0, idx1)]\n",
    "    subgraph_attributes = [\n",
    "        [attributes[0][i] for i in idx0],\n",
    "        [attributes[1][i] for i in idx1]\n",
    "    ]\n",
    "    return subgraph_A, subgraph_attributes\n",
    "\n",
    "## check what happens when repeated partition in row and column\n",
    "def find_connected_components(A, attributes, n_components = 1):\n",
    "    \"\"\"\n",
    "    Find connected components of a multipartite graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix\n",
    "        The adjacency matrix of the graph.\n",
    "    attributes : list of lists\n",
    "        The attributes of the nodes. The first list contains the attributes\n",
    "        of the nodes in rows. The second list contains\n",
    "        the attributes of the nodes in the columns.\n",
    "    n_components : int\n",
    "        The number of components to be found.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cc_As : list of scipy.sparse.csr_matrix\n",
    "        The adjacency matrices of the connected components.\n",
    "    cc_attributes : list of lists\n",
    "        The attributes of the nodes of the connected components. The first list contains\n",
    "        the attributes of the nodes in the rows. The second\n",
    "        list contains the attributes of the nodes in the columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    A_dilation = symmetric_dilation(A)\n",
    "    _, cc = connected_components(A_dilation)\n",
    "    cc = [cc[:A.shape[0]], cc[A.shape[0]:]]\n",
    "    if n_components == None:\n",
    "        n_components = np.max(cc) + 1\n",
    "    else:\n",
    "        cc_As = []\n",
    "        cc_attributes = []\n",
    "        for i in range(n_components):\n",
    "            idx0 = np.where(cc[0] == i)[0]\n",
    "            idx1 = np.where(cc[1] == i)[0]\n",
    "            store_cc_A, store_cc_attributes = subgraph_idx(A,attributes, idx0, idx1)\n",
    "            cc_As.append(store_cc_A)\n",
    "            cc_attributes.append(store_cc_attributes)\n",
    "        if len(cc_As) == 1:\n",
    "            cc_As = cc_As[0]\n",
    "            cc_attributes = cc_attributes[0]\n",
    "        return cc_As, cc_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_networkx(A, attributes, symmetric=None):\n",
    "    \"\"\" \n",
    "    Convert a multipartite graph to a networkx graph.\n",
    "    \"\"\"\n",
    "    if symmetric == None:\n",
    "        if is_symmetric(A):\n",
    "            symmetric = True\n",
    "        else:\n",
    "            symmetric = False\n",
    "    if symmetric:\n",
    "        G_nx = nx.Graph(A)\n",
    "        nx.set_node_attributes(G_nx, {i: a for i, a in enumerate(attributes[0])})\n",
    "        return G_nx\n",
    "    else:\n",
    "        n0 = len(attributes[0])\n",
    "        n1 = len(attributes[1])\n",
    "        G_nx = nx.Graph(symmetric_dilation(A))\n",
    "        nx.set_node_attributes(G_nx, {i: a for i, a in enumerate(attributes[0])})\n",
    "        nx.set_node_attributes(G_nx, {i + n0: a for i, a in enumerate(attributes[1])})\n",
    "        nx.set_node_attributes(G_nx, {i: {'bipartite': 0} for i in range(n0)})\n",
    "        nx.set_node_attributes(G_nx, {i + n0: {'bipartite': 1} for i in range(n1)})\n",
    "        return G_nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_inv_sqrt(a, tol=1e-12):\n",
    "    \"\"\"\n",
    "    Compute the inverse square root of an array, ignoring division by zero.\n",
    "    \"\"\"\n",
    "    with np.errstate(divide=\"ignore\"):\n",
    "        b = 1 / np.sqrt(a)\n",
    "    b[np.isinf(b)] = 0\n",
    "    b[a < tol] = 0\n",
    "    return b\n",
    "\n",
    "def to_laplacian(A, regulariser=0):\n",
    "    \"\"\"\n",
    "    Convert an adjacency matrix to a Laplacian matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix\n",
    "        The adjacency matrix.\n",
    "    regulariser : float\n",
    "        The regulariser to be added to the degrees of the nodes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    L : scipy.sparse.csr_matrix\n",
    "        The Laplacian matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    left_degrees = np.reshape(np.asarray(A.sum(axis=1)), (-1,))\n",
    "    right_degrees = np.reshape(np.asarray(A.sum(axis=0)), (-1,))\n",
    "    if regulariser == 'auto':\n",
    "        regulariser = np.mean(np.concatenate((left_degrees, right_degrees)))\n",
    "    left_degrees_inv_sqrt = safe_inv_sqrt(left_degrees + regulariser)\n",
    "    right_degrees_inv_sqrt = safe_inv_sqrt(right_degrees + regulariser)\n",
    "    L = sparse.diags(left_degrees_inv_sqrt) @ A @ sparse.diags(right_degrees_inv_sqrt)\n",
    "    return L\n",
    "\n",
    "def embed(A, d = 10, matrix = 'adjacency', regulariser = 0):\n",
    "    \"\"\" \n",
    "    Embed a graph using the laplacian or adjacency matrix.  \n",
    "\n",
    "    Parameters  \n",
    "    ----------  \n",
    "    A : scipy.sparse.csr_matrix  \n",
    "        The adjacency matrix of the graph.  \n",
    "    d : int \n",
    "        The dimension of the embedding.\n",
    "    matrix : str    \n",
    "        The matrix to be used for embedding.\n",
    "    regulariser : float \n",
    "        The regulariser to be added to the degrees of the nodes (if matrix = 'laplacian' used).    \n",
    "\n",
    "    Returns \n",
    "    ------- \n",
    "    left_embedding : numpy.ndarray \n",
    "        The left embedding of the graph.    \n",
    "    right_embedding : numpy.ndarray \n",
    "        The right embedding of the graph.    \n",
    "    \"\"\"\n",
    "\n",
    "    if matrix == 'laplacian':\n",
    "        L = to_laplacian(A, regulariser)\n",
    "        u, s, vT = svds(L, d)\n",
    "    else:\n",
    "        u, s, vT = svds(A, d)\n",
    "    o = np.argsort(s[::-1])\n",
    "    left_embedding = u[:,o] @ np.diag(np.sqrt(s[o]))\n",
    "    right_embedding = vT.T[:,o] @ np.diag(np.sqrt(s[o]))                      \n",
    "    return left_embedding, right_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## post embedding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_symmetric(m):\n",
    "    \"\"\"Check if a sparse matrix is symmetric\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : array or sparse matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    check : bool\n",
    "        The check result.\n",
    "\n",
    "    \"\"\"\n",
    "    if m.shape[0] != m.shape[1]:\n",
    "        return False\n",
    "\n",
    "    if not isinstance(m, sparse.coo_matrix):\n",
    "        m = sparse.coo_matrix(m)\n",
    "\n",
    "    r, c, v = m.row, m.col, m.data\n",
    "    tril_no_diag = r > c\n",
    "    triu_no_diag = c > r\n",
    "\n",
    "    if triu_no_diag.sum() != tril_no_diag.sum():\n",
    "        return False\n",
    "\n",
    "    rl = r[tril_no_diag]\n",
    "    cl = c[tril_no_diag]\n",
    "    vl = v[tril_no_diag]\n",
    "    ru = r[triu_no_diag]\n",
    "    cu = c[triu_no_diag]\n",
    "    vu = v[triu_no_diag]\n",
    "\n",
    "    sortl = np.lexsort((cl, rl))\n",
    "    sortu = np.lexsort((ru, cu))\n",
    "    vl = vl[sortl]\n",
    "    vu = vu[sortu]\n",
    "\n",
    "    check = np.allclose(vl, vu)\n",
    "\n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_subspaces(embedding, attributes):\n",
    "    \"\"\"\n",
    "    Recover the subspaces for each partition from an embedding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : numpy.ndarray\n",
    "        The embedding of the graph.\n",
    "    attributes : list of lists\n",
    "        The attributes of the nodes. The first list contains the attributes\n",
    "        of the nodes in rows. The second list contains\n",
    "        the attributes of the nodes in the columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    partition_embeddings : dict\n",
    "        The embeddings of the partitions.\n",
    "    partition_attributes : dict\n",
    "        The attributes of the nodes in the partitions.\n",
    "    \"\"\"\n",
    "\n",
    "    partitions = list(set([x['partition'] for x in attributes]))\n",
    "    partition_embeddings = {}\n",
    "    partition_attributes = {}\n",
    "    for p in partitions:\n",
    "        p_embedding, p_attributes = select(embedding,attributes, {'partition': p})\n",
    "        Y = p_embedding\n",
    "        u, s, vT = linalg.svd(Y, full_matrices=False)\n",
    "        o = np.argsort(s[::-1])\n",
    "        Y = Y @ vT.T[:, o]\n",
    "        partition_embeddings[p] = Y\n",
    "        partition_attributes[p] = p_attributes\n",
    "    return partition_embeddings, partition_attributes\n",
    "\n",
    "def select(embedding, attributes, select_attributes):\n",
    "    \"\"\"\n",
    "    Select portion of embedding and attributes associated with a set of attributes.\n",
    "    \"\"\"\n",
    "    if not isinstance(select_attributes, list):\n",
    "        select_attributes = [select_attributes]\n",
    "    which_nodes = list()\n",
    "    for attributes_dict in select_attributes:\n",
    "        for a, v in attributes_dict.items():\n",
    "            if not isinstance(v, list):\n",
    "                v = [v]\n",
    "        which_nodes_by_attribute = [[i for i, y in enumerate(attributes) if y[a] in v] for a, v in attributes_dict.items()]\n",
    "        which_nodes.append(list(set.intersection(*map(set, which_nodes_by_attribute))))\n",
    "    which_nodes = list(set().union(*which_nodes))\n",
    "    selected_X = embedding[which_nodes, :]\n",
    "    selected_attributes = [attributes[i] for i in which_nodes]\n",
    "    return selected_X, selected_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(X, d):\n",
    "    \"\"\"\n",
    "    Truncate an embedding to a lower dimension.\n",
    "    \"\"\"\n",
    "    Y = X[:, :d]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_correction(X):\n",
    "    \"\"\"\n",
    "    Perform degree correction.\n",
    "    \"\"\"\n",
    "    tol = 1e-12\n",
    "    Y = deepcopy(X)\n",
    "    norms = np.linalg.norm(X, axis=1)\n",
    "    idx = np.where(norms > tol)\n",
    "    Y[idx] = X[idx] / (norms[idx, None])\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33099/960175470.py:3: DtypeWarning: Columns (13,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('/home/ag16115/Documents/phd/codebase_data/activity_data.csv', sep = '\\t', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "# need the activity_data.csv file\n",
    "data = pd.read_csv('/home/ag16115/Documents/phd/codebase_data/brazil/activity_data.csv', sep = '\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making A matrix and attributes\n",
    "A, attributes = graph_from_dataframe(data, [['Company', 'Tender'],['Company', 'Buyer'],['Company', 'Item']], join_token='::')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find subgraph wanted\n",
    "\n",
    "subgraph_attributes = [\n",
    "    [{'partition': 'Company'},{'partition': 'Tender'}],\n",
    "    {'partition': 'Buyer'}\n",
    "]\n",
    "\n",
    "# subgraph_attributes = [\n",
    "#     {'partition': 'Company'},\n",
    "#     {'partition': 'Buyer'}\n",
    "# ]\n",
    "subgraph_A, subgraph_attributes  = find_subgraph(A, attributes,subgraph_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_dilation = symmetric_dilation(subgraph_A)\n",
    "# is_symmetric(A_dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the largest connected component\n",
    "cc_A, cc_attributes = find_connected_components(subgraph_A, subgraph_attributes,n_components = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = to_networkx(cc_A, cc_attributes)\n",
    "# G.number_of_nodes()\n",
    "# G.nodes[1]\n",
    "# list(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get left and right embeddings\n",
    "left_embed, right_embed = embed(cc_A, matrix = 'laplacian')\n",
    "# the attributes associated with left_embed and right_embed are cc_attributes[0] and cc_attributes[1]\n",
    "left_attributes = cc_attributes[0]\n",
    "right_attributes = cc_attributes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_embeddings, partition_attributes = recover_subspaces(left_embed,left_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_embeddings['Company'] = degree_correction(partition_embeddings['Company'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
