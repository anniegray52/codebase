{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.sparse.linalg import svds\n",
    "import networkx as nx\n",
    "from copy import deepcopy\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multipartite Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook contains the functions used to deal with multipartite graphs - taken from the work we did with microsoft. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally - the functions will take in and out 2 things: \n",
    "- the matrix or the embedding\n",
    "- attributes: two lists, each a dictionary containing information about the rows/columns respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with own path to data folder:\n",
    "# path = 'path_to_data_folder'\n",
    "# path = '/home/ag16115/Documents/phd/codebase_data/brazil'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finished functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_from_dataframe(data, partition_pairs, time_col=None, join_token='::'):\n",
    "    \"\"\" \n",
    "    Create a DMP graph from a dataframe.    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame or list of pandas.DataFrame\n",
    "        The data to be used to create the graph. If a list of dataframes is \n",
    "        passed, each dataframe is used to create a separate graph.\n",
    "    partition_pairs : list of lists\n",
    "        The partition pairs to be used to create the graph. Each element of\n",
    "        the list is a list of two elements, which are the names of the\n",
    "        partitions to be joined.\n",
    "    time_col : str or list of str\n",
    "        The name of the column containing the time information. If a list of\n",
    "        strings is passed, each string is the name of the column containing\n",
    "        the time information for each dataframe in data.\n",
    "    join_token : str\n",
    "        The token used to join the partition name and the node name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : scipy.sparse.csr_matrix\n",
    "        The adjacency matrix of the graph.  \n",
    "    attributes : list of lists\n",
    "        The attributes of the nodes. The first list contains the attributes\n",
    "        of the nodes that do not change over time. The second list contains\n",
    "        the attributes of the nodes that change over time.\n",
    "    \"\"\"\n",
    "    # Ensure data and partition_pairs are in list format\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    if not isinstance(partition_pairs[0][0], list):\n",
    "        partition_pairs = [partition_pairs]\n",
    "\n",
    "    # Handle the case when time_col is None\n",
    "    if time_col is None:\n",
    "        time_col = [None] * len(data)\n",
    "    elif isinstance(time_col, str):\n",
    "        time_col = [time_col] * len(data)\n",
    "\n",
    "    edge_list = create_edge_list(data, partition_pairs, time_col, join_token)\n",
    "    nodes, partitions, times, node_ids, time_ids = extract_node_time_info(edge_list, join_token)\n",
    "\n",
    "    edge_list = transform_edge_data(edge_list, node_ids, time_ids, len(nodes))\n",
    "    A = create_adjacency_matrix(edge_list, len(nodes), len(times))\n",
    "    attributes = create_node_attributes(nodes, partitions, times, len(nodes), len(times))\n",
    "\n",
    "    return A.tocsr(), attributes\n",
    "\n",
    "def create_edge_list(data, partition_pairs, time_col, join_token):\n",
    "    edge_list = []\n",
    "    for data0, partition_pairs0, time_col0 in zip(data, partition_pairs, time_col):\n",
    "        for partition_pair in partition_pairs0:\n",
    "            if time_col0:\n",
    "                pair_data = data0[partition_pair + [time_col0]].drop_duplicates()\n",
    "                pair_data['T'] = pair_data[time_col0]\n",
    "                pair_data = pair_data.drop(columns=[time_col0])\n",
    "            else:\n",
    "                pair_data = data0[partition_pair].drop_duplicates()\n",
    "                pair_data['T'] = np.nan\n",
    "            pair_data.columns = ['V1', 'V2', 'T']\n",
    "            pair_data['V1'] = [f\"{partition_pair[0]}{join_token}{x}\" for x in pair_data['V1']]\n",
    "            pair_data['V2'] = [f\"{partition_pair[1]}{join_token}{x}\" for x in pair_data['V2']]\n",
    "            pair_data['P1'] = partition_pair[0]\n",
    "            pair_data['P2'] = partition_pair[1]\n",
    "            edge_list.append(pair_data)\n",
    "            print(partition_pair)\n",
    "    return pd.concat(edge_list)\n",
    "\n",
    "def extract_node_time_info(edge_list, join_token):\n",
    "    nodes = sorted(set(edge_list['V1']).union(edge_list['V2']))\n",
    "    partitions = [node.split(join_token)[0] for node in nodes]\n",
    "    times = sorted(set(edge_list['T'].unique()))\n",
    "    # times = sorted(set(edge_list['T']))\n",
    "    node_ids = {node: idx for idx, node in enumerate(nodes)}\n",
    "    time_ids = {time: idx for idx, time in enumerate(times)}\n",
    "    return nodes, partitions, times, node_ids, time_ids\n",
    "\n",
    "def transform_edge_data(edge_list, node_ids, time_ids, n_nodes):\n",
    "    edge_list['V_ID1'] = edge_list['V1'].map(node_ids)\n",
    "    edge_list['V_ID2'] = edge_list['V2'].map(node_ids)\n",
    "    edge_list['T_ID'] = edge_list['T'].map(time_ids)\n",
    "    edge_list['X_ID1'] = edge_list['T_ID'] * n_nodes + edge_list['V_ID1']\n",
    "    edge_list['X_ID2'] = edge_list['T_ID'] * n_nodes + edge_list['V_ID2']\n",
    "    return edge_list\n",
    "\n",
    "def create_adjacency_matrix(edge_list, n_nodes, n_times):\n",
    "    row_indices = pd.concat([edge_list['V_ID1'], edge_list['V_ID2']])\n",
    "    col_indices = pd.concat([edge_list['X_ID2'], edge_list['X_ID1']])\n",
    "    values = np.ones(2 * len(edge_list))\n",
    "    return sparse.coo_matrix((values, (row_indices, col_indices)), shape=(n_nodes, n_nodes * n_times))\n",
    "\n",
    "def create_node_attributes(nodes, partitions, times, n_nodes, n_times):\n",
    "    time_attrs = np.repeat(times, n_nodes)\n",
    "    attributes = [\n",
    "        [{'name': name, 'partition': partition, 'time': np.nan} for name, partition in zip(nodes, partitions)],\n",
    "        [{'name': name, 'partition': partition, 'time': time} for name, partition, time in zip(nodes * n_times, partitions * n_times, time_attrs)]\n",
    "    ]\n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realise this may seem like a convoluted way to do this but it'll make it easier to add time element later\n",
    "def find_subgraph(A, attributes, subgraph_attributes):\n",
    "    \"\"\"\n",
    "    Find a subgraph of a multipartite graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix\n",
    "        The adjacency matrix of the multipartite graph.\n",
    "    attributes : list of lists\n",
    "        The attributes of the nodes. The first list contains the attributes\n",
    "        of the nodes in rows. The second list contains\n",
    "        the attributes of the nodes in the columns.\n",
    "    subgraph_attributes : list of lists\n",
    "        The attributes of the nodes of the wanted in the subgraph. The first list contains\n",
    "        the attributes of the nodes wanted in the rows. The second\n",
    "        list contains the attributes of the nodes wanted in the column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    subgraph_A : scipy.sparse.csr_matrix\n",
    "        The adjacency matrix of the subgraph.\n",
    "    subgraph_attributes : list of lists\n",
    "        The attributes of the nodes of the subgraph. The first list contains\n",
    "        the attributes of the nodes in the rows. The second\n",
    "        list contains the attributes of the nodes in the columns.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(subgraph_attributes[0], list):\n",
    "        subgraph_attributes[0] = [subgraph_attributes[0]]\n",
    "\n",
    "    if not isinstance(subgraph_attributes[1], list):\n",
    "        subgraph_attributes[1] = [subgraph_attributes[1]]\n",
    "\n",
    "    # find the indices of the rows with required attributes\n",
    "    subgraph_node_indices_row = []\n",
    "    for node_idx, node_attributes in enumerate(attributes[0]):\n",
    "        for each_subgraph_attributes in subgraph_attributes[0]:\n",
    "            matched = True\n",
    "            for key, value in each_subgraph_attributes.items():\n",
    "                if key not in node_attributes or node_attributes[key] != value:\n",
    "                    matched = False\n",
    "                    break\n",
    "            if matched:\n",
    "                subgraph_node_indices_row.append(node_idx)\n",
    "\n",
    "    # find the indices of the columns with required attributes\n",
    "    subgraph_node_indices_col = []\n",
    "    for node_idx, node_attributes in enumerate(attributes[1]):\n",
    "        for each_subgraph_attributes in subgraph_attributes[1]:\n",
    "            matched = True\n",
    "            for key, value in each_subgraph_attributes.items():\n",
    "                if key not in node_attributes or node_attributes[key] != value:\n",
    "                    matched = False\n",
    "                    break\n",
    "            if matched:\n",
    "                subgraph_node_indices_col.append(node_idx)\n",
    "\n",
    "    # create subgraph and subgraph attributes\n",
    "    subgraph_A = A[np.ix_(subgraph_node_indices_row,\n",
    "                          subgraph_node_indices_col)]\n",
    "    subgraph_attributes = [[attributes[0][i] for i in subgraph_node_indices_row], [\n",
    "        attributes[1][i] for i in subgraph_node_indices_col]]\n",
    "\n",
    "    return subgraph_A, subgraph_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_matrix(m, n = None):\n",
    "    \"\"\"\n",
    "    Create a zero matrix.\n",
    "    \"\"\"\n",
    "    if n == None:\n",
    "        n = m\n",
    "    M = sparse.coo_matrix(([],([],[])),shape = (m,n))\n",
    "    return M\n",
    "\n",
    "def symmetric_dilation(M):\n",
    "    \"\"\"\n",
    "    Dilate a matrix to a symmetric matrix.\n",
    "    \"\"\"\n",
    "    m, n = M.shape\n",
    "    D = sparse.vstack([sparse.hstack([zero_matrix(m), M]), sparse.hstack([M.T, zero_matrix(n)])])\n",
    "    return D\n",
    "\n",
    "def subgraph_idx(A, attributes, idx0, idx1):\n",
    "    \"\"\"\n",
    "    Find a subgraph of a multipartite graph by indices.\n",
    "    \"\"\"  \n",
    "    subgraph_A = A[np.ix_(idx0, idx1)]\n",
    "    subgraph_attributes = [\n",
    "        [attributes[0][i] for i in idx0],\n",
    "        [attributes[1][i] for i in idx1]\n",
    "    ]\n",
    "    return subgraph_A, subgraph_attributes\n",
    "\n",
    "## check what happens when repeated partition in row and column\n",
    "def find_connected_components(A, attributes, n_components = 1):\n",
    "    \"\"\"\n",
    "    Find connected components of a multipartite graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix\n",
    "        The adjacency matrix of the graph.\n",
    "    attributes : list of lists\n",
    "        The attributes of the nodes. The first list contains the attributes\n",
    "        of the nodes in rows. The second list contains\n",
    "        the attributes of the nodes in the columns.\n",
    "    n_components : int\n",
    "        The number of components to be found.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cc_As : list of scipy.sparse.csr_matrix\n",
    "        The adjacency matrices of the connected components.\n",
    "    cc_attributes : list of lists\n",
    "        The attributes of the nodes of the connected components. The first list contains\n",
    "        the attributes of the nodes in the rows. The second\n",
    "        list contains the attributes of the nodes in the columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    A_dilation = symmetric_dilation(A)\n",
    "    _, cc = connected_components(A_dilation)\n",
    "    cc = [cc[:A.shape[0]], cc[A.shape[0]:]]\n",
    "    if n_components == None:\n",
    "        n_components = np.max(cc) + 1\n",
    "    else:\n",
    "        cc_As = []\n",
    "        cc_attributes = []\n",
    "        for i in range(n_components):\n",
    "            idx0 = np.where(cc[0] == i)[0]\n",
    "            idx1 = np.where(cc[1] == i)[0]\n",
    "            store_cc_A, store_cc_attributes = subgraph_idx(A,attributes, idx0, idx1)\n",
    "            cc_As.append(store_cc_A)\n",
    "            cc_attributes.append(store_cc_attributes)\n",
    "        if len(cc_As) == 1:\n",
    "            cc_As = cc_As[0]\n",
    "            cc_attributes = cc_attributes[0]\n",
    "        return cc_As, cc_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_networkx(A, attributes, symmetric=None):\n",
    "    \"\"\" \n",
    "    Convert a multipartite graph to a networkx graph.\n",
    "    \"\"\"\n",
    "    if symmetric == None:\n",
    "        if is_symmetric(A):\n",
    "            symmetric = True\n",
    "        else:\n",
    "            symmetric = False\n",
    "    if symmetric:\n",
    "        G_nx = nx.Graph(A)\n",
    "        nx.set_node_attributes(G_nx, {i: a for i, a in enumerate(attributes[0])})\n",
    "        return G_nx\n",
    "    else:\n",
    "        n0 = len(attributes[0])\n",
    "        n1 = len(attributes[1])\n",
    "        G_nx = nx.Graph(symmetric_dilation(A))\n",
    "        nx.set_node_attributes(G_nx, {i: a for i, a in enumerate(attributes[0])})\n",
    "        nx.set_node_attributes(G_nx, {i + n0: a for i, a in enumerate(attributes[1])})\n",
    "        nx.set_node_attributes(G_nx, {i: {'bipartite': 0} for i in range(n0)})\n",
    "        nx.set_node_attributes(G_nx, {i + n0: {'bipartite': 1} for i in range(n1)})\n",
    "        return G_nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_inv_sqrt(a, tol=1e-12):\n",
    "    \"\"\"\n",
    "    Compute the inverse square root of an array, ignoring division by zero.\n",
    "    \"\"\"\n",
    "    with np.errstate(divide=\"ignore\"):\n",
    "        b = 1 / np.sqrt(a)\n",
    "    b[np.isinf(b)] = 0\n",
    "    b[a < tol] = 0\n",
    "    return b\n",
    "\n",
    "def to_laplacian(A, regulariser=0):\n",
    "    \"\"\"\n",
    "    Convert an adjacency matrix to a Laplacian matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix\n",
    "        The adjacency matrix.\n",
    "    regulariser : float\n",
    "        The regulariser to be added to the degrees of the nodes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    L : scipy.sparse.csr_matrix\n",
    "        The Laplacian matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    left_degrees = np.reshape(np.asarray(A.sum(axis=1)), (-1,))\n",
    "    right_degrees = np.reshape(np.asarray(A.sum(axis=0)), (-1,))\n",
    "    if regulariser == 'auto':\n",
    "        regulariser = np.mean(np.concatenate((left_degrees, right_degrees)))\n",
    "    left_degrees_inv_sqrt = safe_inv_sqrt(left_degrees + regulariser)\n",
    "    right_degrees_inv_sqrt = safe_inv_sqrt(right_degrees + regulariser)\n",
    "    L = sparse.diags(left_degrees_inv_sqrt) @ A @ sparse.diags(right_degrees_inv_sqrt)\n",
    "    return L\n",
    "\n",
    "def embed(A, d = 10, matrix = 'adjacency', regulariser = 0):\n",
    "    \"\"\" \n",
    "    Embed a graph using the laplacian or adjacency matrix.  \n",
    "\n",
    "    Parameters  \n",
    "    ----------  \n",
    "    A : scipy.sparse.csr_matrix  \n",
    "        The adjacency matrix of the graph.  \n",
    "    d : int \n",
    "        The dimension of the embedding.\n",
    "    matrix : str    \n",
    "        The matrix to be used for embedding.\n",
    "    regulariser : float \n",
    "        The regulariser to be added to the degrees of the nodes (if matrix = 'laplacian' used).    \n",
    "\n",
    "    Returns \n",
    "    ------- \n",
    "    left_embedding : numpy.ndarray \n",
    "        The left embedding of the graph.    \n",
    "    right_embedding : numpy.ndarray \n",
    "        The right embedding of the graph.    \n",
    "    \"\"\"\n",
    "\n",
    "    if matrix == 'laplacian':\n",
    "        L = to_laplacian(A, regulariser)\n",
    "        u, s, vT = svds(L, d)\n",
    "    else:\n",
    "        u, s, vT = svds(A, d)\n",
    "    o = np.argsort(s[::-1])\n",
    "    left_embedding = u[:,o] @ np.diag(np.sqrt(s[o]))\n",
    "    right_embedding = vT.T[:,o] @ np.diag(np.sqrt(s[o]))                      \n",
    "    return left_embedding, right_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## post embedding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_symmetric(m):\n",
    "    \"\"\"Check if a sparse matrix is symmetric\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : array or sparse matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    check : bool\n",
    "        The check result.\n",
    "\n",
    "    \"\"\"\n",
    "    if m.shape[0] != m.shape[1]:\n",
    "        return False\n",
    "\n",
    "    if not isinstance(m, sparse.coo_matrix):\n",
    "        m = sparse.coo_matrix(m)\n",
    "\n",
    "    r, c, v = m.row, m.col, m.data\n",
    "    tril_no_diag = r > c\n",
    "    triu_no_diag = c > r\n",
    "\n",
    "    if triu_no_diag.sum() != tril_no_diag.sum():\n",
    "        return False\n",
    "\n",
    "    rl = r[tril_no_diag]\n",
    "    cl = c[tril_no_diag]\n",
    "    vl = v[tril_no_diag]\n",
    "    ru = r[triu_no_diag]\n",
    "    cu = c[triu_no_diag]\n",
    "    vu = v[triu_no_diag]\n",
    "\n",
    "    sortl = np.lexsort((cl, rl))\n",
    "    sortu = np.lexsort((ru, cu))\n",
    "    vl = vl[sortl]\n",
    "    vu = vu[sortu]\n",
    "\n",
    "    check = np.allclose(vl, vu)\n",
    "\n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_subspaces(embedding, attributes):\n",
    "    \"\"\"\n",
    "    Recover the subspaces for each partition from an embedding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : numpy.ndarray\n",
    "        The embedding of the graph.\n",
    "    attributes : list of lists\n",
    "        The attributes of the nodes. The first list contains the attributes\n",
    "        of the nodes in rows. The second list contains\n",
    "        the attributes of the nodes in the columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    partition_embeddings : dict\n",
    "        The embeddings of the partitions.\n",
    "    partition_attributes : dict\n",
    "        The attributes of the nodes in the partitions.\n",
    "    \"\"\"\n",
    "\n",
    "    partitions = list(set([x['partition'] for x in attributes]))\n",
    "    partition_embeddings = {}\n",
    "    partition_attributes = {}\n",
    "    for p in partitions:\n",
    "        p_embedding, p_attributes = select(embedding,attributes, {'partition': p})\n",
    "        Y = p_embedding\n",
    "        u, s, vT = linalg.svd(Y, full_matrices=False)\n",
    "        o = np.argsort(s[::-1])\n",
    "        Y = Y @ vT.T[:, o]\n",
    "        partition_embeddings[p] = Y\n",
    "        partition_attributes[p] = p_attributes\n",
    "    return partition_embeddings, partition_attributes\n",
    "\n",
    "def select(embedding, attributes, select_attributes):\n",
    "    \"\"\"\n",
    "    Select portion of embedding and attributes associated with a set of attributes.\n",
    "    \"\"\"\n",
    "    if not isinstance(select_attributes, list):\n",
    "        select_attributes = [select_attributes]\n",
    "    which_nodes = list()\n",
    "    for attributes_dict in select_attributes:\n",
    "        for a, v in attributes_dict.items():\n",
    "            if not isinstance(v, list):\n",
    "                v = [v]\n",
    "        which_nodes_by_attribute = [[i for i, y in enumerate(attributes) if y[a] in v] for a, v in attributes_dict.items()]\n",
    "        which_nodes.append(list(set.intersection(*map(set, which_nodes_by_attribute))))\n",
    "    which_nodes = list(set().union(*which_nodes))\n",
    "    selected_X = embedding[which_nodes, :]\n",
    "    selected_attributes = [attributes[i] for i in which_nodes]\n",
    "    return selected_X, selected_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(X, d):\n",
    "    \"\"\"\n",
    "    Truncate an embedding to a lower dimension.\n",
    "    \"\"\"\n",
    "    Y = X[:, :d]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_correction(X):\n",
    "    \"\"\"\n",
    "    Perform degree correction.\n",
    "    \"\"\"\n",
    "    tol = 1e-12\n",
    "    Y = deepcopy(X)\n",
    "    norms = np.linalg.norm(X, axis=1)\n",
    "    idx = np.where(norms > tol)\n",
    "    Y[idx] = X[idx] / (norms[idx, None])\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains information about the procurement process in Brazil. Each row contains information about a tender with information:\n",
    "- Tender: tender id\n",
    "- Period: time \n",
    "- Buyer: who is funding\n",
    "- Item: what the tender is abour \n",
    "- Company: who has bid for the tender\n",
    "- bidder_win: whether the bid was won or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ag16115/Documents/phd/codebase/dynamic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "# need the activity_data.csv file\n",
    "data = pd.read_csv(path + '/ia-primary-school-proximity-attr.edges', sep = ',', on_bad_lines='skip', header = None)\n",
    "# data = pd.read_csv(path + '/activity_data.csv', sep = '\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename columns   \n",
    "data.columns = ['V1', 'V2', 'T', 'L1', 'L2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['H'] = [int(int(t)/(60*60)) for t in list(data['T'])]\n",
    "data['D'] = [int(int(t)/(60*60*24)) for t in list(data['T'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['T1'] = [10*int(i/24) + i%24 - 8 for i in list(data['H'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V1', 'V2']\n"
     ]
    }
   ],
   "source": [
    "# making A matrix and attributes\n",
    "A, attributes = graph_from_dataframe(data, [['V1', 'V2']], time_col = 'T1', join_token='::')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<480x9600 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 52702 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find subgraph wanted\n",
    "\n",
    "subgraph_attributes = [\n",
    "    [{'partition': 'Company'},{'partition': 'Tender'}],\n",
    "    {'partition': 'Buyer'}\n",
    "]\n",
    "\n",
    "# subgraph_attributes = [\n",
    "#     {'partition': 'Company'},\n",
    "#     {'partition': 'Buyer'}\n",
    "# ]\n",
    "subgraph_A, subgraph_attributes  = find_subgraph(A, attributes,subgraph_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_dilation = symmetric_dilation(subgraph_A)\n",
    "# is_symmetric(A_dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the largest connected component\n",
    "cc_A, cc_attributes = find_connected_components(subgraph_A, subgraph_attributes,n_components = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = to_networkx(cc_A, cc_attributes)\n",
    "# G.number_of_nodes()\n",
    "# G.nodes[1]\n",
    "# list(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get left and right embeddings\n",
    "left_embed, right_embed = embed(cc_A, matrix = 'laplacian')\n",
    "# the attributes associated with left_embed and right_embed are cc_attributes[0] and cc_attributes[1]\n",
    "left_attributes = cc_attributes[0]\n",
    "right_attributes = cc_attributes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_embeddings, partition_attributes = recover_subspaces(left_embed,left_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_embeddings['Company'] = degree_correction(partition_embeddings['Company'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
